"""
Turkish Government Intelligence Hub - Utility Functions
YardÄ±mcÄ± fonksiyonlar
"""

import logging
import os
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional

from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma  # âœ… Yeni paket (deprecation fix)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

import config

# Suppress ChromaDB telemetry
logging.getLogger("chromadb.telemetry.product.posthog").setLevel(logging.CRITICAL)
os.environ["ANONYMIZED_TELEMETRY"] = "False"

# ============================================
# LOGGING SETUP
# ============================================

logging.basicConfig(
    level=config.LOG_LEVEL,
    format=config.LOG_FORMAT,
    handlers=[
        logging.FileHandler(config.LOG_FILE, encoding="utf-8"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)

# ============================================
# PDF PROCESSING FUNCTIONS
# ============================================


def load_pdf(pdf_path: Path) -> List[Document]:
    """
    PDF dosyasÄ±nÄ± yÃ¼kle

    Args:
        pdf_path: PDF dosya yolu

    Returns:
        List[Document]: YÃ¼klenmiÅŸ sayfalar
    """
    try:
        logger.info(f"PDF yÃ¼kleniyor: {pdf_path.name}")
        loader = PyPDFLoader(str(pdf_path))
        pages = loader.load()
        logger.info(f"âœ… {len(pages)} sayfa yÃ¼klendi")
        return pages
    except FileNotFoundError:
        logger.error(f"âŒ PDF bulunamadÄ±: {pdf_path}")
        raise
    except Exception as e:
        logger.error(f"âŒ PDF yÃ¼kleme hatasÄ±: {str(e)}")
        raise


def chunk_documents(
    pages: List[Document],
    chunk_size: int = config.CHUNK_SIZE,
    chunk_overlap: int = config.CHUNK_OVERLAP,
) -> List[Document]:
    """
    DÃ¶kÃ¼manlarÄ± chunk'lara bÃ¶l

    Args:
        pages: PDF sayfalarÄ±
        chunk_size: Chunk boyutu
        chunk_overlap: Chunk overlap

    Returns:
        List[Document]: Chunk'lanmÄ±ÅŸ dÃ¶kÃ¼manlar
    """
    logger.info(
        f"Metin chunk'lara bÃ¶lÃ¼nÃ¼yor (size={chunk_size}, overlap={chunk_overlap})..."
    )

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len
    )

    chunks = text_splitter.split_documents(pages)
    logger.info(f"âœ… {len(chunks)} chunk oluÅŸturuldu")

    return chunks


# ============================================
# EMBEDDING FUNCTIONS
# ============================================


def load_embeddings(model_name: str = config.EMBEDDING_MODEL) -> HuggingFaceEmbeddings:
    """
    TÃ¼rkÃ§e embedding modelini yÃ¼kle

    Args:
        model_name: HuggingFace model adÄ±

    Returns:
        HuggingFaceEmbeddings: YÃ¼klenmiÅŸ embedding modeli
    """
    logger.info(f"Embedding modeli yÃ¼kleniyor: {model_name}")

    try:
        embeddings = HuggingFaceEmbeddings(model_name=model_name)
        logger.info("âœ… Embedding modeli hazÄ±r")
        return embeddings
    except Exception as e:
        logger.error(f"âŒ Embedding yÃ¼kleme hatasÄ±: {str(e)}")
        raise


# ============================================
# VECTOR DATABASE FUNCTIONS
# ============================================


def create_vectorstore(
    chunks: List[Document],
    embeddings: HuggingFaceEmbeddings,
    persist_dir: Path,
    collection_name: str = config.COLLECTION_NAME,
) -> Chroma:
    """
    Vector database oluÅŸtur ve kaydet.

    Args:
        chunks: Chunk'lanmÄ±ÅŸ dÃ¶kÃ¼manlar.
        embeddings: Embedding modeli.
        persist_dir: Kaydedilecek dizin.
        collection_name: Koleksiyon adÄ±.

    Returns:
        Chroma: OluÅŸturulan vector database.
    """
    logger.info(
        f"Vector database oluÅŸturuluyor: {persist_dir} (Collection: {collection_name})"
    )

    try:
        persist_dir.mkdir(parents=True, exist_ok=True)

        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory=str(persist_dir),
            collection_name=collection_name,
        )

        logger.info(f"âœ… Vector database kaydedildi: {persist_dir}")
        return vectorstore

    except Exception as e:
        logger.error(f"âŒ Vector DB oluÅŸturma hatasÄ±: {str(e)}")
        raise


def add_to_vectorstore(
    chunks: List[Document],
    vectorstore: Chroma,
) -> None:
    """
    Mevcut bir vector database'e yeni dÃ¶kÃ¼manlar ekler.

    Args:
        chunks: Eklenecek chunk'lanmÄ±ÅŸ dÃ¶kÃ¼manlar.
        vectorstore: Hedef vector database.
    """
    logger.info(f"Vector database'e {len(chunks)} yeni chunk ekleniyor...")
    try:
        vectorstore.add_documents(chunks)
        logger.info("âœ… DÃ¶kÃ¼manlar baÅŸarÄ±yla eklendi.")
    except Exception as e:
        logger.error(f"âŒ DÃ¶kÃ¼man ekleme hatasÄ±: {str(e)}")
        raise


def load_vectorstore(
    persist_dir: Path,
    embeddings: HuggingFaceEmbeddings,
    collection_name: str = config.COLLECTION_NAME,
) -> Chroma:
    """
    HazÄ±r vector database'i yÃ¼kle.

    Args:
        persist_dir: Vector DB dizini.
        embeddings: Embedding modeli.
        collection_name: Koleksiyon adÄ±.

    Returns:
        Chroma: YÃ¼klenmiÅŸ vector database.
    """
    logger.info(
        f"Vector database yÃ¼kleniyor: {persist_dir} (Collection: {collection_name})"
    )

    try:
        if not persist_dir.exists():
            raise FileNotFoundError(f"Vector DB bulunamadÄ±: {persist_dir}")

        vectorstore = Chroma(
            persist_directory=str(persist_dir),
            embedding_function=embeddings,
            collection_name=collection_name,
        )

        logger.info("âœ… Vector database yÃ¼klendi")
        return vectorstore

    except Exception as e:
        logger.error(f"âŒ Vector DB yÃ¼kleme hatasÄ±: {str(e)}")
        raise


# ============================================
# SEARCH FUNCTIONS
# ============================================


def search_similar_docs(
    vectorstore: Chroma,
    question: str,
    top_k: int = config.TOP_K,
    filter_metadata: Optional[Dict[str, Any]] = None,
) -> Tuple[str, List[float], List[Document]]:
    """
    Benzer dÃ¶kÃ¼manlarÄ± bul (opsiyonel metadata filtresi ile).

    Args:
        vectorstore: Vector database.
        question: KullanÄ±cÄ± sorusu.
        top_k: KaÃ§ chunk getirileceÄŸi.
        filter_metadata: Metadata filtresi (Ã–rn: {"party": "CHP"}).

    Returns:
        Tuple[str, List[float], List[Document]]: (BirleÅŸtirilmiÅŸ metin, Benzerlik skorlarÄ±, Orjinal dÃ¶kÃ¼manlar).
    """
    logger.info(f"Arama yapÄ±lÄ±yor: '{question}' (Filtre: {filter_metadata})")

    try:
        relevant_docs_with_scores = vectorstore.similarity_search_with_score(
            question, k=top_k, filter=filter_metadata
        )

        if not relevant_docs_with_scores:
            return "", [], []

        relevant_docs = [doc for doc, score in relevant_docs_with_scores]
        relevant_chunks = [doc.page_content for doc in relevant_docs]
        context = "\n\n".join(relevant_chunks)
        scores = [score for doc, score in relevant_docs_with_scores]

        logger.info(f"âœ… {len(relevant_docs)} chunk bulundu, skorlar: {scores}")

        return context, scores, relevant_docs

    except Exception as e:
        logger.error(f"âŒ Arama hatasÄ±: {str(e)}")
        raise


# ============================================
# VALIDATION FUNCTIONS
# ============================================


def validate_pdf_exists(party: str) -> bool:
    """
    Parti PDF'inin var olup olmadÄ±ÄŸÄ±nÄ± kontrol et

    Args:
        party: Parti kÄ±sa adÄ± (CHP, AKP, etc.)

    Returns:
        bool: PDF var mÄ±?
    """
    pdf_path = config.PARTY_PDFS.get(party)

    if pdf_path is None:
        logger.error(f"âŒ Parti bulunamadÄ±: {party}")
        return False

    if not pdf_path.exists():
        logger.warning(f"âš ï¸ PDF bulunamadÄ±: {pdf_path}")
        return False

    return True


def get_available_parties() -> List[str]:
    """
    Mevcut PDF'leri olan partileri listele

    Returns:
        List[str]: Mevcut partiler
    """
    available = []

    for party, pdf_path in config.PARTY_PDFS.items():
        if pdf_path.exists():
            available.append(party)

    return available


def get_prepared_parties() -> List[str]:
    """
    Unified Vector DB'si hazÄ±r olan veya verisi bulunan partileri listeler.

    Returns:
        List[str]: HazÄ±r olan partilerin listesi.
    """
    if config.UNIFIED_VECTOR_DB.exists():
        # EÄŸer unified DB varsa, PDF'i olan tÃ¼m partilerin hazÄ±r olduÄŸunu varsayÄ±yoruz.
        # Daha kesin Ã§Ã¶zÃ¼m iÃ§in DB iÃ§indeki metadata'larÄ± sorgulamak gerekebilir.
        return get_available_parties()

    return []


# ============================================
# DISPLAY FUNCTIONS
# ============================================


def print_header(text: str, width: int = 60):
    """BaÅŸlÄ±k yazdÄ±r"""
    print("\n" + "=" * width)
    print(text.center(width))
    print("=" * width)


def print_party_info(party: str):
    """Parti bilgilerini yazdÄ±r"""
    info = config.PARTY_INFO.get(party)
    if info:
        print(f"\n{info.get('hex_color', '#0066FF')} {info['name']} ({info['short']})")
        print(f"ğŸŒ Website: {info['website']}")


# ============================================
# DYNAMIC PARTY DISCOVERY
# ============================================


def discover_parties() -> Dict[str, Path]:
    """
    data/ klasÃ¶rÃ¼ndeki tÃ¼m PDF'leri otomatik keÅŸfeder.

    Ã–rnek:
        chp.pdf â†’ CHP
        dem.pdf â†’ DEM
        iyip.pdf â†’ IYIP

    Returns:
        Dict[str, Path]: Parti adÄ± (anahtar) ve PDF dosya yolu (deÄŸer) sÃ¶zlÃ¼ÄŸÃ¼.
    """
    pdf_dir = config.DATA_DIR
    parties = {}

    if not pdf_dir.exists():
        logger.warning(f"âš ï¸ Data klasÃ¶rÃ¼ bulunamadÄ±: {pdf_dir}")
        return parties

    for pdf_file in pdf_dir.glob("*.pdf"):
        party_name = pdf_file.stem.upper()  # chp.pdf â†’ CHP
        parties[party_name] = pdf_file
        logger.info(f"âœ… KeÅŸfedilen parti: {party_name}")

    if not parties:
        logger.warning("âš ï¸ HiÃ§ PDF bulunamadÄ±!")
        return {}

    logger.info(f"âœ… Toplam {len(parties)} parti keÅŸfedildi")
    return parties


def get_or_create_vectorstore(party: str, embeddings: HuggingFaceEmbeddings) -> Chroma:
    """
    Parti iÃ§in vector database'i yÃ¼kle veya oluÅŸtur

    - EÄŸer DB varsa: yÃ¼kle (fast, ~2s)
    - EÄŸer DB yoksa: otomatik oluÅŸtur (one-time, ~3-5 min)

    Args:
        party: Parti adÄ± (CHP, AKP, DEM, vb.)
        embeddings: Embedding modeli

    Returns:
        Chroma: Vector database

    Raises:
        FileNotFoundError: PDF bulunamadÄ±
    """
    db_path = config.VECTOR_DB_DIR / f"{party.lower()}_db"
    pdf_path = config.DATA_DIR / f"{party.lower()}.pdf"

    # âœ… DB zaten varsa, yÃ¼kle (fast)
    if db_path.exists():
        logger.info(f"âœ… {party} verileri cache'den yÃ¼kleniyor...")
        return load_vectorstore(db_path, embeddings)

    # âŒ DB yoksa ve PDF varsa, otomatik oluÅŸtur
    if not pdf_path.exists():
        error_msg = f"âŒ PDF bulunamadÄ±: {pdf_path}"
        logger.error(error_msg)
        raise FileNotFoundError(error_msg)

    logger.info(f"ğŸ”„ {party} iÃ§in veri hazÄ±rlanÄ±yor (ilk kez, ~3-5 min)...")

    try:
        # Pipeline: PDF â†’ Chunks â†’ Embeddings â†’ Vector DB
        logger.info(f"  1ï¸âƒ£ PDF yÃ¼kleniyor: {party}")
        pages = load_pdf(pdf_path)

        logger.info("  2ï¸âƒ£ Chunk'lara bÃ¶lÃ¼nÃ¼yor...")
        chunks = chunk_documents(pages)

        logger.info("  3ï¸âƒ£ Vector DB oluÅŸturuluyor...")
        vectorstore = create_vectorstore(chunks, embeddings, db_path)

        logger.info(f"âœ… {party} hazÄ±rlama tamamlandÄ±!")
        return vectorstore

    except Exception as e:
        logger.error(f"âŒ {party} hazÄ±rlama hatasÄ±: {str(e)}")
        raise
