"""
Turkish Government Intelligence Hub - Utility Functions
YardÄ±mcÄ± fonksiyonlar
"""

import logging
from pathlib import Path
from typing import List, Tuple
from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma  # âœ… Yeni paket (deprecation fix)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

import config

# ============================================
# LOGGING SETUP
# ============================================

logging.basicConfig(
    level=config.LOG_LEVEL,
    format=config.LOG_FORMAT,
    handlers=[
        logging.FileHandler(config.LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================
# PDF PROCESSING FUNCTIONS
# ============================================

def load_pdf(pdf_path: Path) -> List[Document]:
    """
    PDF dosyasÄ±nÄ± yÃ¼kle

    Args:
        pdf_path: PDF dosya yolu

    Returns:
        List[Document]: YÃ¼klenmiÅŸ sayfalar
    """
    try:
        logger.info(f"PDF yÃ¼kleniyor: {pdf_path.name}")
        loader = PyPDFLoader(str(pdf_path))
        pages = loader.load()
        logger.info(f"âœ… {len(pages)} sayfa yÃ¼klendi")
        return pages
    except FileNotFoundError:
        logger.error(f"âŒ PDF bulunamadÄ±: {pdf_path}")
        raise
    except Exception as e:
        logger.error(f"âŒ PDF yÃ¼kleme hatasÄ±: {str(e)}")
        raise


def chunk_documents(
    pages: List[Document],
    chunk_size: int = config.CHUNK_SIZE,
    chunk_overlap: int = config.CHUNK_OVERLAP
) -> List[Document]:
    """
    DÃ¶kÃ¼manlarÄ± chunk'lara bÃ¶l

    Args:
        pages: PDF sayfalarÄ±
        chunk_size: Chunk boyutu
        chunk_overlap: Chunk overlap

    Returns:
        List[Document]: Chunk'lanmÄ±ÅŸ dÃ¶kÃ¼manlar
    """
    logger.info(f"Metin chunk'lara bÃ¶lÃ¼nÃ¼yor (size={chunk_size}, overlap={chunk_overlap})...")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len
    )

    chunks = text_splitter.split_documents(pages)
    logger.info(f"âœ… {len(chunks)} chunk oluÅŸturuldu")

    return chunks

# ============================================
# EMBEDDING FUNCTIONS
# ============================================

def load_embeddings(model_name: str = config.EMBEDDING_MODEL) -> HuggingFaceEmbeddings:
    """
    TÃ¼rkÃ§e embedding modelini yÃ¼kle

    Args:
        model_name: HuggingFace model adÄ±

    Returns:
        HuggingFaceEmbeddings: YÃ¼klenmiÅŸ embedding modeli
    """
    logger.info(f"Embedding modeli yÃ¼kleniyor: {model_name}")

    try:
        embeddings = HuggingFaceEmbeddings(model_name=model_name)
        logger.info("âœ… Embedding modeli hazÄ±r")
        return embeddings
    except Exception as e:
        logger.error(f"âŒ Embedding yÃ¼kleme hatasÄ±: {str(e)}")
        raise

# ============================================
# VECTOR DATABASE FUNCTIONS
# ============================================

def create_vectorstore(
    chunks: List[Document],
    embeddings: HuggingFaceEmbeddings,
    persist_dir: Path
) -> Chroma:
    """
    Vector database oluÅŸtur ve kaydet

    Args:
        chunks: Chunk'lanmÄ±ÅŸ dÃ¶kÃ¼manlar
        embeddings: Embedding modeli
        persist_dir: Kaydedilecek dizin

    Returns:
        Chroma: Vector database
    """
    logger.info(f"Vector database oluÅŸturuluyor: {persist_dir}")

    try:
        # Dizini oluÅŸtur
        persist_dir.mkdir(parents=True, exist_ok=True)

        # Vector DB oluÅŸtur
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory=str(persist_dir)
        )

        logger.info(f"âœ… Vector database kaydedildi: {persist_dir}")
        return vectorstore

    except Exception as e:
        logger.error(f"âŒ Vector DB oluÅŸturma hatasÄ±: {str(e)}")
        raise


def load_vectorstore(
    persist_dir: Path,
    embeddings: HuggingFaceEmbeddings
) -> Chroma:
    """
    HazÄ±r vector database'i yÃ¼kle

    Args:
        persist_dir: Vector DB dizini
        embeddings: Embedding modeli

    Returns:
        Chroma: YÃ¼klenmiÅŸ vector database
    """
    logger.info(f"Vector database yÃ¼kleniyor: {persist_dir}")

    try:
        if not persist_dir.exists():
            raise FileNotFoundError(f"Vector DB bulunamadÄ±: {persist_dir}")

        vectorstore = Chroma(
            persist_directory=str(persist_dir),
            embedding_function=embeddings
        )

        logger.info(f"âœ… Vector database yÃ¼klendi")
        return vectorstore

    except Exception as e:
        logger.error(f"âŒ Vector DB yÃ¼kleme hatasÄ±: {str(e)}")
        raise

# ============================================
# SEARCH FUNCTIONS
# ============================================

def search_similar_docs(
    vectorstore: Chroma,
    question: str,
    top_k: int = config.TOP_K
) -> Tuple[str, List[float]]:
    """
    Benzer dÃ¶kÃ¼manlarÄ± bul

    Args:
        vectorstore: Vector database
        question: KullanÄ±cÄ± sorusu
        top_k: KaÃ§ chunk getireceÄŸiz

    Returns:
        Tuple[str, List[float]]: (context, similarity_scores)
    """
    logger.info(f"Arama yapÄ±lÄ±yor: '{question}'")

    try:
        relevant_docs = vectorstore.similarity_search_with_score(question, k=top_k)

        relevant_chunks = [doc.page_content for doc, score in relevant_docs]
        context = "\n\n".join(relevant_chunks)
        scores = [score for doc, score in relevant_docs]

        logger.info(f"âœ… {len(relevant_docs)} chunk bulundu, skorlar: {scores}")

        return context, scores

    except Exception as e:
        logger.error(f"âŒ Arama hatasÄ±: {str(e)}")
        raise

# ============================================
# VALIDATION FUNCTIONS
# ============================================

def validate_pdf_exists(party: str) -> bool:
    """
    Parti PDF'inin var olup olmadÄ±ÄŸÄ±nÄ± kontrol et

    Args:
        party: Parti kÄ±sa adÄ± (CHP, AKP, etc.)

    Returns:
        bool: PDF var mÄ±?
    """
    pdf_path = config.PARTY_PDFS.get(party)

    if pdf_path is None:
        logger.error(f"âŒ Parti bulunamadÄ±: {party}")
        return False

    if not pdf_path.exists():
        logger.warning(f"âš ï¸ PDF bulunamadÄ±: {pdf_path}")
        return False

    return True


def get_available_parties() -> List[str]:
    """
    Mevcut PDF'leri olan partileri listele

    Returns:
        List[str]: Mevcut partiler
    """
    available = []

    for party, pdf_path in config.PARTY_PDFS.items():
        if pdf_path.exists():
            available.append(party)

    return available


def get_prepared_parties() -> List[str]:
    """
    Vector DB'si hazÄ±r olan partileri listele

    Returns:
        List[str]: HazÄ±r partiler
    """
    prepared = []

    for party, db_path in config.PARTY_VECTOR_DBS.items():
        if db_path.exists():
            prepared.append(party)

    return prepared

# ============================================
# DISPLAY FUNCTIONS
# ============================================

def print_header(text: str, width: int = 60):
    """BaÅŸlÄ±k yazdÄ±r"""
    print("\n" + "="*width)
    print(text.center(width))
    print("="*width)


def print_party_info(party: str):
    """Parti bilgilerini yazdÄ±r"""
    info = config.PARTY_INFO.get(party)
    if info:
        print(f"\n{info['color']} {info['name']} ({info['short']})")
        print(f"ğŸŒ Website: {info['website']}")